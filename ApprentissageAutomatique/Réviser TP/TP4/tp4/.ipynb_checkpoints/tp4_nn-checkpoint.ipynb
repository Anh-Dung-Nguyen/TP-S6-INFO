{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import dataset_loader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercises 4 - Neural Networks with Pytorch\n",
    "\n",
    "The goal of this lab exercise is to help you learn how to use Pytorch, one of the most widely used neural network library.\n",
    "\n",
    "It is important the you read the documentation to understand how to use Pytorch functions, what kind of transformation they apply etc. You have to take time to read it carefully to understand what you are doing.\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.html\n",
    "- https://pytorch.org/docs/stable/torch.html\n",
    "\n",
    "# 1. Pytorch basics\n",
    "\n",
    "Instead of manipulating numpy arrays, we will manipulate pytorch tensors.\n",
    "A lot of things are defined in the same way, except the you can use autograd!\n",
    "\n",
    "Note that when using pytorch and the autograd mechanism, you want to avoid in-place operations, see the example below (in-place operations are important for parameter initialization, but we will skip this in this lab).\n",
    "It is easy to identify in-place operations: their function name ends with an underscore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2121, 0.6757, 0.6768, 0.9208],\n",
      "        [0.4830, 0.2097, 0.5354, 0.5437]])\n",
      "\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# create a tensor of shape (2, 4) containing random values.\n",
    "# by default, it will be a float tensor.\n",
    "\n",
    "t = torch.rand(2, 4)\n",
    "\n",
    "# you can also create a tensor full of 0 or 1\n",
    "t_zeros = torch.zeros(2, 4)\n",
    "t_ones = torch.ones(2, 4)\n",
    "\n",
    "print(t)\n",
    "print()\n",
    "print(t_zeros)\n",
    "print()\n",
    "print(t_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access properties of a tensor like its shape or the type of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the tensor: torch.Size([2, 4])\n",
      "type of elements: torch.float32\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2, 4)\n",
    "print(\"shape of the tensor:\", t.shape)\n",
    "print(\"type of elements:\", t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([1.4500, 0.0000, 0.0000])\n",
      "\n",
      "torch.int64\n",
      "tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# you can specify the type of the data contained in the tensor\n",
    "\n",
    "t = torch.zeros(3, dtype=torch.float32)\n",
    "t[0] = 1.45\n",
    "\n",
    "print(t.dtype)\n",
    "print(t)\n",
    "print()\n",
    "\n",
    "t_long = torch.zeros(3, dtype=torch.long)\n",
    "t_long[0] = 1.45\n",
    "\n",
    "print(t_long.dtype)\n",
    "print(t_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8196, 0.2919, 0.4896])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# there also exists *_like functions that creates a tensor with exactly\n",
    "# the same properties as its argument (shape, type, etc)\n",
    "\n",
    "t2 = torch.rand_like(t)\n",
    "t2_zeros = torch.zeros_like(t)\n",
    "t2_ones = torch.ones_like(t)\n",
    "\n",
    "print(t2)\n",
    "print(t2_zeros)\n",
    "print(t2_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also initialize the tensor with values\n",
    "t_long = torch.LongTensor([0,1,10,20])\n",
    "print(t_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient computation\n",
    "\n",
    "Now, let's turn to the serious stuff: gradient computation! :)\n",
    "\n",
    "When you compute values with Pytorch tensors, the library implicitly builds a computation graph that can be used by the backpropagation algorithm to compute gradients.\n",
    "However, the graph is build only if there are tensors that have their attribute `requires_grad` set to `True`.\n",
    "In other word, `t.requires_grad==True` indicates that we want the gradient with respect to this tensor when using the backpropagation algorithm. The gradient will not be computed for tensors for which `requires_grad` is set to `False`.\n",
    "\n",
    "The backpropagation algorithm can simply be called using the `backward()` method on a tensor (which should be the output tensor, for example the loss value, and usually it should be scalar value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2, 10)\n",
    "\n",
    "# by default no gradient will be required for t :(\n",
    "print(t.requires_grad)\n",
    "\n",
    "# so we ask for it explicitly (note the underscore: in place operation!)\n",
    "t.requires_grad_(True)\n",
    "print(t.requires_grad)\n",
    "\n",
    "# We can also set this to true at creation\n",
    "t = torch.rand(2, 10, requires_grad=True)\n",
    "print(t.requires_grad)\n",
    "print()\n",
    "\n",
    "# now, let's do a stupid operation and compute the gradient.\n",
    "# This sum over all element of t,\n",
    "# it return a tensor with a single value.\n",
    "z = t.sum()\n",
    "print(\"z shape:\", z.shape)  # this will look strange!\n",
    "print(\"z requires grad?\", z.requires_grad)\n",
    "print()\n",
    "\n",
    "# backpropagation!\n",
    "z.backward()\n",
    "\n",
    "# print the gradient of t,\n",
    "# it should be a vector full of 1, do you understand why?\n",
    "print()\n",
    "print(\"Gradient:\")\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if I call backward a second time, it will accumulate the gradient\n",
    "# so it will be a tensort full of 2\n",
    "z.backward()\n",
    "print(t.grad)\n",
    "print()\n",
    "\n",
    "# we can reset the gradient of the tensor to zero\n",
    "# => note the in-place operation\n",
    "t.grad.zero_()\n",
    "print(t.grad)\n",
    "print()\n",
    "\n",
    "# and if we backprop again, it will be a tensor full of one again\n",
    "z.backward()\n",
    "print(t.grad)\n",
    "\n",
    "# and this highlight one of the major source of bug in pytorch:\n",
    "# Do not forget to reset your gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the lectures, we saw that a neural network library builds a computation graph in order to be able to run the backpropagation algorithm (to compute gradients).\n",
    "The next examples show how it is stored in pytorch variables.\n",
    "\n",
    "In the next example, we just sum two values, and then inspect the associated computation graph.\n",
    "\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Question:</b>\n",
    "    Run the code below and compare the result when calling the function grad_fn with argument 1 and argument 2.\n",
    "    How do you interpret this result?\n",
    "    What are these values? How are they computed and why?\n",
    "    What is the argument given to this function?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print(\"a = %f\" % a.item())\n",
    "print(\"b = %f\" % b.item())\n",
    "print(\"c = a + b = %f\" % c.item())\n",
    "print()\n",
    "\n",
    "print(\"The function to call during backward for variable c is:\", c.grad_fn)\n",
    "print()\n",
    "\n",
    "print(\"Calling this function with argument 1\")\n",
    "print(c.grad_fn(torch.tensor([1]))) \n",
    "print()\n",
    "\n",
    "print(\"Calling this function with argument 2\")\n",
    "print(c.grad_fn(torch.tensor([2]))) \n",
    "print()\n",
    "\n",
    "print(\"The next functions that must be called during backpropagation are:\")\n",
    "print(c.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do the same computation, but without requiring a gradient for b.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Question:</b> What difference do you notice in the computation graph? How do you explain this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=False)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print(\"a = %f\" % a.item())\n",
    "print(\"b = %f\" % b.item())\n",
    "print(\"c = a + b = %f\" % c.item())\n",
    "print()\n",
    "\n",
    "print(\"The function to call during backward for variable c is :\", c.grad_fn)\n",
    "print()\n",
    "\n",
    "print(\"Calling this function with argument 1\")\n",
    "print(c.grad_fn(torch.tensor([1]))) \n",
    "print()\n",
    "\n",
    "print(\"Calling this function with argument 2\")\n",
    "print(c.grad_fn(torch.tensor([2]))) \n",
    "print()\n",
    "\n",
    "print(\"The next functions that must be called during backpropagation are:\")\n",
    "print(c.grad_fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see the impact of in-place operation on the backpropagation algorithm.\n",
    "In the two following example, we first do a simple operation (addition or multication),\n",
    "then modify in-place the value of tensor \"a\", and then call the back propagation algorithm.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Question:</b>\n",
    "For the addition, modifying \"a\" in-place does not break the backpropagation algorithm.\n",
    "When happens for the multiplication? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "# the torch.no_grad function can be used to create an environment\n",
    "# in which no computation graph will be created.\n",
    "# it is mandatory to modify \"leaf\" variables that require a gradient.\n",
    "with torch.no_grad():\n",
    "    # modify the value of variable \"a\"\n",
    "    # [:] is required so the modification is in place,\n",
    "    # as a = 0 would just make \"a\" reference the value 0,\n",
    "    # instead of modifying the tensor referenced by \"a\"\n",
    "    a[:] = 0\n",
    "\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "c = a * b\n",
    "\n",
    "# the torch.no_grad function can be used to create an environment\n",
    "# in which no computation graph will be created.\n",
    "# it is mandatory to modify \"leaf\" variables that require a gradient.\n",
    "with torch.no_grad():\n",
    "    # modify the value of variable \"a\"\n",
    "    # [:] is required so the modification is in place,\n",
    "    # as a = 0 would just make \"a\" reference the value 0,\n",
    "    # instead of modifying the tensor referenced by \"a\"\n",
    "    a[:] = 0\n",
    "\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. First Example : Neural Network for Binary Classification\n",
    "\n",
    "In this first example, we will build a two neural networks for binary classification:\n",
    "\n",
    "1. a simple linear classifier, as in the previous lab exercise, but here we will use the autograd mechanism/backpropagation algorithm and all other cool stuff in pytorch to learn the parameters.\n",
    "2. a multilinear perceptron that learns a non-linear transformation of the data\n",
    "\n",
    "## 3.1. Linear Model with Pytorch\n",
    "\n",
    "We must first create the data. We use the same function as in the previous lab exercise.\n",
    "\n",
    "We will store the data in list of dictionnaries,\n",
    "each dictionnary is a data point, with two keys:\n",
    "\n",
    "- `input`: the input tensor\n",
    "- `label`: the gold output/label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(mu + res * sigmas)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1], sigmas2=[0.1, 0.1]):\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]\n",
    "\n",
    "X, y = rand_bi_gauss(n1=150, n2=150)\n",
    "\n",
    "# transform into lists of dictionnaries\n",
    "separable_binary_data = list()\n",
    "for i in range(len(y)):\n",
    "    separable_binary_data.append({\n",
    "        \"input\": torch.tensor(X[i], dtype=torch.float32),\n",
    "        # for the label, we need to convert to the 0/1 format instead of -1/1,\n",
    "        # moreover, the label must be a long\n",
    "        \"label\": torch.tensor((y[i] + 1) // 2, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "# split into train/dev/test\n",
    "separable_binary_data_train = separable_binary_data[:200]\n",
    "separable_binary_data_dev = separable_binary_data[200:250]\n",
    "separable_binary_data_test = separable_binary_data[250:]\n",
    "\n",
    "# show the first training datapoint\n",
    "print(\"Example of datapoint:\")\n",
    "print(separable_binary_data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Modules\n",
    "\n",
    "The parameter of your network must be encapsulated in a special class called Parameter: https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
    "This is very important so that Pytorch knows what are the parameters of the network,\n",
    "that it must update these parameters when learning, etc.\n",
    "\n",
    "However, Pytorch comes with a lot of modules already made!\n",
    "They are in the package torch.nn that we often just rename as nn during import (see the first cell of this notebook).\n",
    "You can directly use these modules, and therefore you may not need to create and store the parameters of your network yourself.\n",
    "\n",
    "For exemple, the `nn.Linear` class represent a simple linear projection, including its parameters.\n",
    "The documentation is available here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear layer that expects an input of dimension 10,\n",
    "# and returns an output of dimension 20\n",
    "linear = nn.Linear(10, 20)\n",
    "\n",
    "# The parameter of the layer are stored\n",
    "# in attributes weight and bias,\n",
    "# we can check their type and dimension\n",
    "\n",
    "print(\"Type of linear.weight:\", type(linear.weight))\n",
    "print(\"Type of linear.bias:\", type(linear.bias))\n",
    "print()\n",
    "print(\"Shape of linear.weight:\", linear.weight.shape)\n",
    "print(\"Shape of linear.bias:\", linear.bias.shape)\n",
    "print()\n",
    "print(\"Does the parameters require gradient by default?\")\n",
    "print(\"- weight:\", linear.weight.requires_grad)\n",
    "print(\"- bias:\", linear.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Linear` class is not a subtype of `nn.Parameter`. It is a small neural network module.\n",
    "\n",
    "Therefore, is must inherit from `nn.Module`, as should all small parts of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(linear))\n",
    "print(isinstance(linear, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has tons a build-ins neural network components, which are listed here : https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "In this lab exercise, I will directly indicate which one you should use!\n",
    "\n",
    "\n",
    "### Minibatches and Parallel Computation\n",
    "\n",
    "Modern hardware (CPU and GPU) allows for parallel computation via either multi-threading and SIMD instructions (Single Instruction/Multiple Data).\n",
    "One of the reasons of the recevent advances in artificial intelligence is the available of high-end GPUs that allows for massive parallel computation.\n",
    "(Note however that we do not use gamer GPUs for this, a good GPU for machine learning, like a NVIDIA H100 or AMD MI300A, costs ~20.000 euros and you need at least 100 of them to start doing anything serious! and that's only the GPUs :) )\n",
    "\n",
    "As such, it is very important to parallelize computation to fully use your hardware (even on CPU!).\n",
    "The simplest form of parallism is parallelizing the forward and backward pass on all the datapoints of your minibatch.\n",
    "Altough this is far from trivial in the general case,\n",
    "for the simple examples we will study in this lab exercise,\n",
    "it is fully transparent: give the full minibatch of data to you neural network, and it will return outputs for all of them.\n",
    "\n",
    "For example, for a linear model, you can give either:\n",
    "\n",
    "1. a single datapoint in form of a vector of $d$ elements, where $d$ is the dimension of the input\n",
    "2. a minibatch of 10 elements, using a matrix of shape $10 \\times d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get the two input/output pairs\n",
    "input1 = separable_binary_data[0][\"input\"]\n",
    "input2 = separable_binary_data[1][\"input\"]\n",
    "\n",
    "# build a simple linear projection\n",
    "# input dimension\n",
    "d = len(input1)\n",
    "\n",
    "linear = nn.Linear(d, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the output associated with each datapoint\n",
    "output1 = linear(input1)\n",
    "output2 = linear(input2)\n",
    "\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two inputs into a matrix,\n",
    "# where each line is one input\n",
    "all_inputs = torch.cat(\n",
    "    [input1.reshape(1, -1), input2.reshape(1, -1)],\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "print(\"input1:\", input1)\n",
    "print(\"input2:\", input2)\n",
    "print()\n",
    "\n",
    "print(\"concatenated inputs:\")\n",
    "print(all_inputs)\n",
    "print()\n",
    "\n",
    "print(\"all_inputs shape:\", all_inputs.shape)\n",
    "\n",
    "# pass both inputs through the linear layer\n",
    "all_outputs = linear(all_inputs)\n",
    "\n",
    "print(\"all_output shape:\", all_outputs.shape)\n",
    "print()\n",
    "\n",
    "# values must be similar to the case where we passed them one by one!\n",
    "print(all_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Example\n",
    "\n",
    "We can now build and train a linear classifier with Pytorch.\n",
    "In the previous lab exercise, we computed the gradient manually, but we also implemented gradient descent manually!\n",
    "\n",
    "In Pytorch, gradient descent and related methods are provided in the `nn.optim` package.\n",
    "It contains classes that we called \"optimizers\", that can do all the work for use.\n",
    "We will mainly use:\n",
    "\n",
    "- `nn.optim.SGD`: simple stochastic gradient descent,\n",
    "- `Adam`: the Adam method (see slides), that works very well in practice.\n",
    "\n",
    "We need to create an optimizer by indicate the parameters to update (i.e. the parameters of the network) and the learning rate.\n",
    "\n",
    "But first, 3 functions to simplify the process:\n",
    "\n",
    "- `build_batch`: takes as input a list datapoints, and concatenate them into a matrix (inputs) and a vector (outputs)\n",
    "- `eval_binary_classification_network`: evaluate a binary classifier on the given data\n",
    "- `train_binary_classification_network`: train a binary classifier\n",
    "\n",
    "Study carefully the code of these functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a helper function\n",
    "def build_batch(data):\n",
    "    batch_inputs = torch.cat(\n",
    "        [data[\"input\"].reshape(1, -1) for data in data],\n",
    "        dim=0\n",
    "    )\n",
    "\n",
    "    labels = torch.LongTensor([data[\"label\"].reshape(-1) for data in data ])\n",
    "    \n",
    "    return batch_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_classification_network(network, data, minibatch_size):\n",
    "    # pass network in eval mode,\n",
    "    # i.e. if the dropout module is called,\n",
    "    # it won't be applied\n",
    "    network.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    # disable auto-grad as we don't need that during evaluation\n",
    "    # this speed things a little bit + use less memory\n",
    "    with torch.no_grad(): \n",
    "        for first_element in range(0, len(data), minibatch_size):\n",
    "            batch_input, labels = build_batch(data[first_element:first_element + minibatch_size])\n",
    "            logits = network(batch_input)\n",
    "\n",
    "            # compute the prediction for each datapoint\n",
    "            prediction = (logits.squeeze(1) >= 0).to(torch.long)\n",
    "            \n",
    "            # compare prediction to gold and add to the counter\n",
    "            num_correct += (prediction == labels).sum().item()\n",
    "\n",
    "    # the function returns the accuracy\n",
    "    return 100 * num_correct / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_classification_network(network, optimizer, num_epochs, minibatch_size, train_data, dev_data=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: %i / %i\" % (epoch+1, num_epochs))\n",
    "        \n",
    "        # shuffle the train data\n",
    "        # its a good practice to do this at the beginning of each epoch\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "        # pass the network in training mode,\n",
    "        # i.e. dropout will be applied if the dropout module is called\n",
    "        network.train()\n",
    "    \n",
    "        # keep track of the whole dataset loss, to print the information at the end of the epoch\n",
    "        epoch_loss = 0\n",
    "        for first_element in range(0, len(train_data), minibatch_size):\n",
    "            # IMPORTANT\n",
    "            # as gradient is accumulated, we need to set all gradients to 0\n",
    "            # there are several ways of doing that,\n",
    "            # the simplest is to call optimizer.zero_grad()\n",
    "            # that set all parameters tracked by the optimizer to 0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # build our batched input\n",
    "            batch_input, labels = build_batch(train_data[first_element:first_element + minibatch_size])\n",
    "            \n",
    "            # compute the output weights/logits\n",
    "            logits = network(batch_input)\n",
    "            \n",
    "            # compute the loss\n",
    "            # the torch.nn.functional packages (renamed as F) contains many\n",
    "            # useful functions that are not network subpart (neither parameters or modules)\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html\n",
    "            loss = F.binary_cross_entropy_with_logits(logits.squeeze(1), labels.to(logits.dtype), reduction=\"sum\")\n",
    "            epoch_loss += loss.item() # it is very important to use .item(), in order to \"step-out\" from the computation graph\n",
    "            \n",
    "            # compute the gradient\n",
    "            # we take the mean loss\n",
    "            loss = loss / batch_input.shape[0]\n",
    "            loss.backward()\n",
    "    \n",
    "            # update parameters wrt to gradient information!\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"- mean loss: %.2f\" % (epoch_loss / len(train_data)))\n",
    "\n",
    "        # at the end of each epoch we evaluate on dev\n",
    "        if dev_data is not None:\n",
    "            dev_accuracy = eval_binary_classification_network(network, dev_data, minibatch_size)\n",
    "            print(\"- dev acc: %.2f\" % dev_accuracy)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can create a linear classifier and train it in a few lines of code. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our network is a simple linear projection\n",
    "network = nn.Linear(2, 1)\n",
    "\n",
    "# Build the optimizer, i.e. the object that will update parameters\n",
    "# using the gradient information.\n",
    "# \n",
    "# SGD is standard gradient descent, but there are many alternative!\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "#\n",
    "# The first argument of an optimizer is the set of parameters if will update,\n",
    "# we can use network.parameters() to get all the parameters of our network\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "\n",
    "# Note: this problem is so trivial that you should directly have 100% accuracy on dev\n",
    "train_binary_classification_network(\n",
    "    network,\n",
    "    optimizer,\n",
    "    3,\n",
    "    10,\n",
    "    separable_binary_data_train,\n",
    "    separable_binary_data_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Multilinear Perceptron for Binary Classification\n",
    "\n",
    "In this, the goal is that you build a multilayer perceptron. We will rely on a dataset that is not linearly separable.\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_circles_dataset():\n",
    "    X, y = make_circles(n_samples=400, noise=0.1, factor=0.3, random_state=0)\n",
    "\n",
    "    data = list()\n",
    "    for i in range(len(y)):\n",
    "        data.append({\n",
    "            \"input\": torch.tensor(X[i], dtype=torch.float32),\n",
    "            # for the label, we need to convert to the 0/1 format instead of -1/1,\n",
    "            # moreover, the label must be a long\n",
    "            \"label\": torch.tensor(y[i], dtype=torch.long)\n",
    "        })\n",
    "    return data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the data,\n",
    "# divide in train/dev/test\n",
    "# and plot the dataset\n",
    "\n",
    "circles_data = build_circles_dataset()\n",
    "\n",
    "circles_data_train = circles_data[:200]\n",
    "circles_data_dev = circles_data[200:300]\n",
    "circles_data_test = circles_data[300:]\n",
    "\n",
    "plt.scatter(\n",
    "    [d[\"input\"][0].item() for d in circles_data],\n",
    "    [d[\"input\"][1].item() for d in circles_data],\n",
    "    c = [d[\"label\"].item() for d in circles_data]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "In the previous example, we only used a linear classifier, so it was \"easy\".\n",
    "For a more complex neural network like a MLP, we need to create it ourselves in a class. We will create a MLP with two hidden layers and one output layer.\n",
    "\n",
    "**Constructor:** the constructor must create all parameters and submodules. In our case, we will need three different linear projections.\n",
    "\n",
    "**forward method:** the forward method takes as input the data, and must return the output of the computation. In our case, these are class weights compute by the network (so you must not compute the loss here, neither apply softmax to transform weights into probabilities).\n",
    "\n",
    "**Note:** the forward method is never called explicitly, i.e. we never write `network.forward(...)`.\n",
    "Instead, we treat the object as a function and write `network(...)`, which will implicitly called the `forward` function.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Todo:</b>\n",
    "Complete the functions `hidden_repr` and `forward` below.\n",
    "</div>\n",
    "\n",
    "In the code, we will split the forward pass in the neural network into two functions:\n",
    "\n",
    "1. the `hidden_repr` function transform the input using the first two linear projections, where each projection is followed by a non-linear activation function. Here we can use the hyperbolic tangen `F.tanh(...)`\n",
    "2. the `forward` function call hidden_repr to compute the hidden representation, and then apply the output projection.\n",
    "\n",
    "**Note:** Both function must returns their respective results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, proj1_dim, proj2_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.proj1 = nn.Linear(input_dim, proj1_dim)\n",
    "        self.proj2 = nn.Linear(proj1_dim, proj2_dim)\n",
    "        self.proj3 = nn.Linear(proj2_dim, 1)\n",
    "\n",
    "    def hidden_repr(self, input_data):\n",
    "        # TODO: complete this function\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # TODO: complete this function\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our neural network!\n",
    "\n",
    "In the code below, compute the hidden representation for all datapoints before and after training, in order to visualize them below.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Todo:</b>\n",
    "Try different parameter for the training loop: learning rate, number of epochs, minibatch size, etc. What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = BinaryClassificationNetwork(2, 10, 2)\n",
    "\n",
    "test_batch_input, test_batch_labels = build_batch(circles_data_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_before = network.hidden_repr(test_batch_input)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)\n",
    "train_binary_classification_network(network, optimizer, 50, 50, circles_data_train, circles_data_dev)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_after = network.hidden_repr(test_batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Representation\n",
    "\n",
    "The neural network works as follows:\n",
    "\n",
    "1. project the input using non-linear transformation,\n",
    "2. use a linear classifier that takes as input the transformed data.\n",
    "\n",
    "It can do this \"end-to-end\", and learn the parameters of the non-linear transformation and the linear classsifier at the same time.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Question:</b>\n",
    "Compare the hidden representation of all datapoints using the two cells below,\n",
    "    that corresponds to transformed data before and after training.\n",
    "    What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    z_before[:, 0],\n",
    "    z_before[:, 1],\n",
    "    c=test_batch_labels.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    z_after[:, 0],\n",
    "    z_after[:, 1],\n",
    "    c=test_batch_labels.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multilinear Perceptron for Multiclass Classification\n",
    "\n",
    "We will now turn to neural networks for multiclass classification.\n",
    "We will use the MNIST dataset, where the goal is to predict the number written on a picture.\n",
    "\n",
    "### MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the MNIST dataset\n",
    "\n",
    "mnist_path = \"./mnist.pkl.gz\"\n",
    "\n",
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)\n",
    "\n",
    "def build_torch_inputs(data):\n",
    "    x, y = data\n",
    "    ret = list()\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        input_tensor = torch.from_numpy(x[i]).reshape(1, -1)\n",
    "        output_value = torch.tensor([int(y[i])], dtype=torch.long)\n",
    "        \n",
    "        ret.append({\n",
    "            \"input\": input_tensor,\n",
    "            \"label\": output_value\n",
    "        })\n",
    "        \n",
    "    return ret\n",
    "        \n",
    "mnist_train = build_torch_inputs(train_data)\n",
    "mnist_dev = build_torch_inputs(dev_data)\n",
    "mnist_test = build_torch_inputs(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data is a list,\n",
    "# each element is a dictionnary with two keys:\n",
    "# - input_tensor: the input image as a row vector\n",
    "# - output_value: the gold label\n",
    "\n",
    "print(mnist_train[10][\"input\"].shape)\n",
    "print(mnist_train[10][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an example from the dataset\n",
    "\n",
    "index = 900\n",
    "label = mnist_train[index][\"label\"]\n",
    "picture = mnist_train[index][\"input\"]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Training Loop\n",
    "\n",
    "We need to adapt the evaluation and training loop functions for the multiclass classification case.\n",
    "Comments highlight the difference with the binary case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_multiclass_classification_network(network, data, minibatch_size):\n",
    "    network.eval()\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for first_element in range(0, len(data), minibatch_size):\n",
    "            batch_input, labels = build_batch(data[first_element:first_element + minibatch_size])\n",
    "            logits = network(batch_input)\n",
    "\n",
    "            # compute the prediction for each datapoint\n",
    "            # contrary to the binary case,\n",
    "            # here we search for the class of maximum weight\n",
    "            # dim=-1 means that the class weight dimension is the last dimension\n",
    "            prediction = logits.argmax(dim=-1)\n",
    "            \n",
    "            # compare prediction to gold and add to the counter\n",
    "            num_correct += (prediction == labels.reshape(-1)).sum().item()\n",
    "\n",
    "    # the function returns the accuracy\n",
    "    return 100 * num_correct / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiclass_classification_network(network, optimizer, num_epochs, minibatch_size, train_data, dev_data=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: %i / %i\" % (epoch+1, num_epochs))\n",
    "        \n",
    "        random.shuffle(train_data)\n",
    "        network.train()\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        for first_element in range(0, len(train_data), minibatch_size):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_input, labels = build_batch(train_data[first_element:first_element + minibatch_size])\n",
    "            \n",
    "            logits = network(batch_input)\n",
    "            \n",
    "            # compute the loss for multiclass classification.\n",
    "            # We use the multiclass negative log-likelihood loss,\n",
    "            # which is implemented by the cross-entropy function.\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "            #\n",
    "            # (there exists another variant, but which require logit pre-processing, so we use this one instead!)\n",
    "            loss = F.cross_entropy(logits, labels, reduction=\"sum\")\n",
    "            epoch_loss += loss.item() # it is very important to use .item(), in order to \"step-out\" from the computation graph\n",
    "            \n",
    "            loss = loss / batch_input.shape[0]\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"- mean loss: %.2f\" % (epoch_loss / len(train_data)))\n",
    "\n",
    "        # at the end of each epoch we evaluate on dev\n",
    "        if dev_data is not None:\n",
    "            dev_accuracy = eval_multiclass_classification_network(network, dev_data)\n",
    "            print(\"- dev acc: %.2f\" % dev_accuracy)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Definition and Training\n",
    "\n",
    "You can now complete the code below!\n",
    "You can try two neural networks:\n",
    "\n",
    "- LinearMulticlassClassification: a simple linear classifier for multiclass classification\n",
    "- MLPClassification: a MLP with one hidden layer\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>Question:</b>\n",
    "Complete the code of the two neural network. Then, try to train them with different hyperparameters and different optimizers (SGD and Adam). What do you observe?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO\n",
    "        # ...\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # TODO\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = LinearMulticlassClassifier(mnist_train[0][\"input\"].shape[-1], 10)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "train_multiclass_classification_network(\n",
    "    network,\n",
    "    optimizer,\n",
    "    10,\n",
    "    50,\n",
    "    mnist_train,\n",
    "    mnist_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMulticlassClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO\n",
    "        # ...\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \n",
    "        # TODO\n",
    "        # ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLPMulticlassClassifier(mnist_train[0][\"input\"].shape[-1], 100, 10)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "train_multiclass_classification_network(\n",
    "    network,\n",
    "    optimizer,\n",
    "    10,\n",
    "    50,\n",
    "    mnist_train,\n",
    "    mnist_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Next Steps!\n",
    "\n",
    "Now that you know how to build a basic model, there are many things you can do!\n",
    "You will find below a list of modification you can explore.\n",
    "The most important one is dropout, but after that you are free to explore them in any order.\n",
    "\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "<b>TODO:</b>\n",
    "Read until the end, and then implement a neural network with dropout and train it.\n",
    "    Then, if you have time, you can implement the other topics discussed here.\n",
    "</div>\n",
    "\n",
    "### Regularization and Gradient Clipping\n",
    "\n",
    "You can try two types of regularization (they can be combined together):\n",
    "\n",
    "- weight decay: it is a parameter of the optimizer\n",
    "- dropout\n",
    "\n",
    "For dropout, you need to create a dropout layer as part of your network. :)\n",
    "It will be automatically enabled/disabled when you call network.train()/.eval().\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code allows you to observe how dropout works\n",
    "\n",
    "t = torch.ones(2, 4)\n",
    "print(\"initial tensor\")\n",
    "print(t)\n",
    "print()\n",
    "\n",
    "\n",
    "dropout = nn.Dropout(0.5)\n",
    "\n",
    "# activate train mode\n",
    "dropout.train()\n",
    "t2 = dropout(t)\n",
    "\n",
    "print(\"tensor after applying dropout (in train mode)\")\n",
    "print(t2)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"tensor after applying dropout (in eval mode)\")\n",
    "dropout.eval()\n",
    "t3 = dropout(t)\n",
    "print(t3)\n",
    "\n",
    "# WARNING => of course you don't need to directly call train()/eval() on the dropout object in practice,\n",
    "# but instead you call the one of the network that will recursively call it to all\n",
    "# its module attributes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commong trick for training neural networks is gradient clipping: if the norm of the gradient is too big, we rescale the gradient. This trick can be used to prevent exploding gradients and also to make \"too big steps\" in the wrong direction due the use of approximate gradient computation in stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss.backward()  # compute gradient\n",
    "torch.nn.utils.clip_grad_value_(network.parameters(), 5.)  # clip gradient if its norm exceed 5\n",
    "optimizer.step()  # update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Lists and Sequential Modules\n",
    "\n",
    "Sometimes, you need to have a list of submodules.\n",
    "In this very important to never store them in \"raw\" Python list, **you should never do** something like this in you network construction:\n",
    "\n",
    "```\n",
    "self.modules = list()\n",
    "self.modules.append(nn.Linear(10, 20)\n",
    "self.modules.append(nn.Linear(20, 20)\n",
    "```\n",
    "\n",
    "The reason is that Pytorch must know which attributes corresponds to neural network parts/submodules.\n",
    "If you use a Python list, Pytorch will not see them.\n",
    "Instead, you can use a `nn.ModuleList()` object:\n",
    "\n",
    "```\n",
    "self.modules = nn.ModuleList()\n",
    "self.modules.append(nn.Linear(10, 20)\n",
    "self.modules.append(nn.Linear(20, 20)\n",
    "```\n",
    "\n",
    "See the documentation: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "\n",
    "Similar classes exists for dictionnaries, etc.\n",
    "\n",
    "\n",
    "Another useful submodule container is nn.Sequential(): https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "The idea behind a Sequential() object is that it is a list of sub-modules. When you call the object, it will just execute one module after the other, passing as input of the next one the result of the previous one.\n",
    "\n",
    "Here is an example on how to use this to construct a single projection with non-linearity and dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage: we define a projection as a Sequential object\n",
    "seq = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5)  \n",
    ")\n",
    "\n",
    "# batched input\n",
    "inputs = torch.rand(3, 10)\n",
    "\n",
    "# will call successively the 3 subnetworks,\n",
    "# i.e. it will apply linear transformation,\n",
    "# then relu and then dropout\n",
    "outputs = seq(inputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it is a little bit more difficult to create than nn.Sequential() because it doesn't have an append() method... but you can use list comprehension + transform the list as a sequence of argument to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list is a single argument\n",
    "print([1, 2, 3])\n",
    "\n",
    "# here we call print with 3 different arguments,\n",
    "# notice how the output is different\n",
    "print(1, 2, 3)\n",
    "\n",
    "# so, how do we call a function by passing the values\n",
    "# from a list as separate argument?\n",
    "# Well, like this:\n",
    "print(*[1, 2, 3])\n",
    "\n",
    "# notive that this last output is similar to the second one,\n",
    "# and different from the first! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
