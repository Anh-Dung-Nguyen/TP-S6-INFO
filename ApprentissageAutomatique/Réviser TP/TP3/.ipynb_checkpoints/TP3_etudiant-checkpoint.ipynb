{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model for Binary Classification\n",
    "\n",
    "Lab exercise authors : Corro, with original parts by Labeau, Baskiotis, Salmon, Gramfort, Mozharovskyi.\n",
    "\n",
    "For this lab exercise, check the documentation/tutorial of the python librairies that we will use if you need help on how to use a specific function:\n",
    "- http://www.python.org\n",
    "- http://scipy.org\n",
    "- http://www.numpy.org\n",
    "- http://scikit-learn.org/stable/index.html\n",
    "- http://www.loria.fr/~rougier/teaching/matplotlib/matplotlib.html and https://matplotlib.org/stable/users/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this lab exercise, we will work on a first practical application of supervised learning, which is arguably the simplest one: **supervised binary classification**. We will generate artificial data from two different sources, and we will try to learn a classifier that will be able to separate the data, using the a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions and notations :\n",
    "\n",
    "- $\\mathcal{X}$ *input/examples/observations/samples* space, in our case inputs are of the form $\\mathbf{x} = (x_1, x_2, ..., x_d) \\in \\mathbb{R}^d$. We call $x_j$ the value taken by the $j$-th variable of $\\mathbf{x}$, or its $j$-th *feature*.\n",
    "- $\\mathcal{Y}$ is the set of labels, or output space. We are in the **binary** case: there is only two possible labels. We choose to note them $\\{ -1, 1\\}$, which will make things easy by allowing us to work with the ```sign``` function.\n",
    "- $\\mathcal{D}_n = \\{(x_i, y_i), i=1, .., n\\}$ is a dataset containing $n$ examples and their labels. \n",
    "- There exists a probabilistic model governing the generation of our data given i.i.d random variables $X$ and $Y$: $\\forall i \\in \\{ 1, ..., n \\}, (x_i, y_i) \\sim (X, Y)$\n",
    "- We would like to build, from $\\mathcal{D}_n$, a function that we call a *classifier*, $$f: \\mathcal{X} \\rightarrow \\{ -1, 1 \\}$$ which for a new data point $\\mathbf{x}_{new}$ will give a label $f(\\mathbf{x}_{new})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just some useful imports and definitions.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "from matplotlib import rc\n",
    "\n",
    "symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']\n",
    "\n",
    "#rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': False,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating artificial data \n",
    "\n",
    "For our first experiment, and in order to visualize what is happening, we will work with only two features (so, with $d=2$) so that we can plot the data and the classifier. \n",
    "\n",
    "1) Take a look at the function ```rand_gauss(n, mu, sigma)```: this function returns $n$ samples following the multi-dimensional normal distribution, with mean the vector $\\mu =$ ```mu```, and with the covariance matrix $\\Sigma$ being the diagonal matrix of the vector ```sigmas``` $=[ \\sigma_1, \\sigma_2]$. Hence, the matrix $$\\Sigma = \\begin{pmatrix}\n",
    "\\sigma_1&0 \\\\\n",
    "0 &\\sigma_2\n",
    "\\end{pmatrix}$$\n",
    "Now, generate different datasets from the function ```rand_bi_gauss``` function. What does the second output correspond to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    \"\"\"Sample  points from a Gaussian variable.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of samples\n",
    "    mu : mean\n",
    "    sigma : standard deviation\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(mu + res * sigmas)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1],\n",
    "                  sigmas2=[0.1, 0.1]):\n",
    "    \"\"\"Sample points from two Gaussian distributions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first distribution\n",
    "    n2 : number of sample from second distribution\n",
    "    mu1 : center for first distribution\n",
    "    mu2 : center for second distribution\n",
    "    sigma1: std deviation for first distribution\n",
    "    sigma2: std deviation for second distribution\n",
    "    \"\"\"\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# you can try to play with rand_gauss to understand what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Generate a dataset using rand_bi_gauss.\n",
    "# For example:\n",
    "# - 20 datapoints for each class\n",
    "# - use mean [1., 1.] for one class and [-1., -1.] for the other\n",
    "# - you can fix the standard deviation to something like 0.9\n",
    "# Print the generate data\n",
    "#\n",
    "# You should store the data in a matrix X1 and a vector y1\n",
    "# (do not use the np.matrix object for the matrix, just a simple numpy tenshor as returned by the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the data using the ```plot_2d``` function below.\n",
    "\n",
    "What do you observe? Is it linearly separable? (you can try to generate and plot different datasets from the same distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(X, y, a=None, b=None, step=50, alpha_choice=1):\n",
    "    \"\"\"2D dataset data ploting according to labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data features\n",
    "    y : label vector\n",
    "    a, b :(optional) the separating hyperplan parameterize by a and b\n",
    "    alpha_choice : control alpha display parameter\n",
    "    \"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "\n",
    "    for i, label in enumerate(y):\n",
    "        label_num = np.where(labels == label)[0][0]\n",
    "        plt.scatter(X[i, 0], X[i, 1],\n",
    "                    c=np.reshape(cm.tab10(label_num), (1, -1)),\n",
    "                    s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0 - delta0 / 10., max_tot0 + delta0 / 10.])\n",
    "    plt.ylim([min_tot1 - delta1 / 10., max_tot1 + delta1 / 10.])\n",
    "    if a is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -a[0] / a[1] - b / a[1],\n",
    "                  max_tot0 * -a[0] / a[1] - b / a[1]],\n",
    "                 \"k\", alpha=alpha_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your generated data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear classifiers (affine)\n",
    "\n",
    "A **linear classifier** is a classifier associating to each observation $x$ a label in $\\mathcal{Y}$ given its position related to an **affine hyperplane**. Each linear classifier is therefore linked to an affine hyperplan of $\\mathbb{R}^d$, which we define by its **directing vector** (or normal vector) $\\mathbf{a} = (a_1, ..., a_d) \\in \\mathbb{R}^{d}$ and an intercept term $b \\in \\mathbb R$ (in French, *ordonnée à l'origine*). Geometrically, this allows the hyperplane to not go through the origin, and be shifted anywhere in the space. \n",
    "\n",
    "The hyperplane is then given by:\n",
    "$$ H_{\\mathbf{a}, b}=\\left\\{ \\mathbf{x} \\in \\mathbb{R}^{d} : b +\\sum_{j=1}^d a_j x_j=0 \\right\\}. $$\n",
    "\n",
    "In order to classify an observation $\\mathbf{x}$ (i.e, affect a label $1$ or $-1$, we use the function $sign(b +\\sum_{j=1}^d a_j x_j)$, where the $sign$ function is defined as:\n",
    "$$ sign(x)=\n",
    "\\begin{cases}\n",
    " 1,& \\text{ si } x\\geq0,\\\\\n",
    "-1,& \\text{ si } x<0\n",
    "\\end{cases}\n",
    "\\enspace . $$\n",
    "\n",
    "Hence, the function  $\\mathbf{x} \\mapsto sign ( b +\\sum_{j=1}^d a_j x_j )$ is the binary classifier of linear separation defined by $\\mathbf{a}$ and $b$.\n",
    "\n",
    "Training or learning aims to find \"good\" parameters $\\mathbb a$ and $b$ from the training data.\n",
    "That is, we would like to have, on each side of the hyperplane, labels separated into homogenous groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next questions, you should reuse the datasets you obtained previously.\n",
    "\n",
    "**Question:** What is the linear separation given by the perceptron in dimension $d=2$? See if you can find (visually) a good separation for your datasets. When is $b +\\sum_{j=1}^d a_j x_j$ large ? Negative ? Positive ? How can that function be interpreted geometrically ? What does $b$ corresponds to on the linear separation you found on your data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Write the the function ```compute_score(x, a, b)``` that takes, as input, a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and model parameters $\\mathbf{a} \\in \\mathbb{R}^{d}$ and $b \\in \\mathbb R$ and outputs the prediction $b +\\sum_{j=1}^d a_j x_j$. Then, write ```predict_class(x,a,b)``` that outputs the predicted label $sign\\left(b +\\sum_{j=1}^d a_j x_j\\right)$. Apply them to the following example, and display on the same plot your data, the hyperplane, and the two new points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(x, a, b):\n",
    "    \"\"\"Compute score using parameter a and b.\"\"\"\n",
    "    # TODO TODO TODO\n",
    "    return ...\n",
    "\n",
    "\n",
    "def predict_class(x, a, b):\n",
    "    # TODO TODO TODO\n",
    "    \"\"\"Predict a class from at point x and parameters a and b.\"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [1, 1] # Visually, seems like it would make an okay normal vector for a separating hyperplane\n",
    "b1 = 0\n",
    "\n",
    "x_test_1 = [-1, -1]\n",
    "x_test_2 = [1, 1]\n",
    "\n",
    "# Make predictions for these two points and display them, with the data and hyperplane. \n",
    "# - use plot_2d to plot the generate data and the separating hyperplane defined by a1 and b1\n",
    "# - use plt.scatter to plot x_test_1 and x_test_2 (use two different colors using the argment `c`)\n",
    "\n",
    "# TODO TODO TODO\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with the perceptron loss\n",
    "\n",
    "Training with the perceptron loss, also called the **perceptron algorithm**, consists in detecting when there is a mistake, meaning that there is a point that is misclassified, and moving $\\mathbf{a}$ and $b$ towards having this point on the 'right side' of the hyperplane.\n",
    "\n",
    "Given score $w = b +\\sum_{j=1}^d a_j x_j$ and gold output $y$, the perceptron loss is defined as:\n",
    "\n",
    "$$\n",
    "    \\ell(w ; y) = max(0, - yw)\n",
    "$$\n",
    "\n",
    "**Question:** analyse the gradient of this function. When is the gradient null? When is it non null? (you can assume the gradient is 0 when non-differentiable).\n",
    "\n",
    "For a given input/output pair $(\\mathbf{x}, y)$\n",
    "The change in $\\mathbf{a}$ and $b$ for a case when the point is misclassified is described by the following update learning rule (Rosenblatt's):\n",
    "$$\\mathbf{a}\\leftarrow\\mathbf{a}+\\epsilon \\mathbf{x} \\cdot y$$\n",
    "$$b\\leftarrow b+\\epsilon y$$\n",
    "\n",
    "where $\\epsilon$ is a *learning step*, which indicates how much we correct $\\mathbf{a}$ and b. The method is iterative: we will go through all examples we have in our data and update the points accordingly. \n",
    "\n",
    "**Question:** analyse this update rule, and compare it to a gradient step using a minibatch of size 1 and the perceptron loss. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**\n",
    "\n",
    "Then, the algorithm is as follows:\n",
    "\n",
    "- **Data**: \n",
    "    - The observations and their labels $\\mathcal{D}_n=\\{(\\mathbf{x}_i,y_i): 1\\leq i \\leq n\\}$\n",
    "    - The gradient step: $\\epsilon$\n",
    "    - The maximal number of iterations: $\\rm{iter}$\n",
    "\n",
    "- **Returns**: \n",
    "    - $\\mathbf{a}$\n",
    "    - $b$\n",
    "- Randomly initialize $\\mathbf{a}$; initialize $b=0$.\n",
    "- For $k=1$ to $\\rm iter$ \n",
    "    - For $i=1$ to $n$:\n",
    "        - if $(b +\\sum_{j=1}^d a_j [\\mathbf x_i]_j) y_i \\leq 0$\n",
    "            - $\\mathbf{a}\\leftarrow\\mathbf{a}+\\epsilon y_i \\mathbf{x}_i$ \n",
    "            - $b\\leftarrow b+\\epsilon y_i$\n",
    "        \n",
    "5. You have to complete the code for this procedure in the ```perceptron``` function:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, eps, niter, a_ini, b_ini):\n",
    "    \"\"\" Perceptron algorithm:\n",
    "        - x : Data\n",
    "        - y : label\n",
    "        - eps : learning rate\n",
    "        - niter : number of iterations\n",
    "        - a_ini, b_ini : initial weight\n",
    "        \"\"\"\n",
    "    # Keep track of a and b at each iterations - the first one is a_ini and b_ini\n",
    "    all_a = np.zeros((niter, a_ini.size))\n",
    "    all_b = np.zeros(niter)\n",
    "    \n",
    "    all_a[0] = a_ini\n",
    "    all_b[0] = b_ini\n",
    "    \n",
    "    # Implement the learning loop\n",
    "    '''\n",
    "    TODO TODO TODO: Complete here\n",
    "    '''\n",
    "    \n",
    "    return all_a, all_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Test the perceptron algorithm on the following parameters and look at how $\\mathbf{a}$ and $b$ evolves during the iterations. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an epsilon\n",
    "epsilon = 0.2\n",
    "# A number of iterations\n",
    "niter = 10\n",
    "# Initialize a_ini and b_ini\n",
    "std_ini = 1.\n",
    "a_ini = std_ini * np.random.randn(X1.shape[1])\n",
    "b_ini = 0\n",
    "\n",
    "# Use the 'perceptron' function \n",
    "'''\n",
    "Complete here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display on the same figure the evolution of the boundaries according to the iterations (remember that the function \"perceptron\" returns all parameters after each iteration on the dataset). We can use the ```plot_2d``` function and its ```alpha_choice``` argument for that purpose.\n",
    "\n",
    "For this, you can use `np.arange(0., 1., 1. / float(niter))` to generate values for alphas.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a loop and the last argument of plot_2d to plot each hyperplane (at each iteration) \n",
    "# with decreasing transparency\n",
    "'''\n",
    "Complete here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. General loss functions\n",
    "\n",
    "While the perceptron loss/algorithm we saw works geometrically (see the course slides), in general, in order to measure the error associated to an entire dataset $\\mathcal{D}_n$ it is necessary to set a loss function $\\ell:\\mathbb{R}\\times\\mathcal{Y} \\mapsto \\mathbb{R}^+$ which measures the cost $h$ of an error $i$ when predicting an example.\n",
    "The cost that we want to minimize (as a function of $\\mathbf{w}$) is $\\mathbb{E}_{\\mathbf x} \\left[ \\ell(b +\\sum_{j=1}^d a_j x_j,y)\\right]$, the expectation of the loss function on all the data. Three loss functions are commonly used and defined below :\n",
    "- the hinge error (*charnière* in French): $HingeLoss(w ; y) = \\max(0,1-y w)$, or margin loss with a margin of 1.\n",
    "- the negative log-likelihood loss: $NLL(w ; y) = log(1 + exp(-yw))$\n",
    "- the quadratic error: $MSELoss(w; y) = (y-w)^2$.\n",
    "\n",
    "**Question:** What is the NLL loss when y=1 and y=-1? What can we conclude when comparing to the NLL loss we saw in the lecture? (remember that in the lecture, we assumed y was either 0 or 1)\n",
    "\n",
    "The purpose here is to study these different loss functions. \n",
    "You must first implement these losses, together with their gradient wrt model parameters $\\mathbf a$ and $b$.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you can do them one by one instead of the three at once\n",
    "# (i.e. write the functions associated with one loss, and then train a model with it)\n",
    "\n",
    "def hinge_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n",
    "\n",
    "def nll_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n",
    "\n",
    "def quadratic_error_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n",
    "\n",
    "# the gradient functions must return two values,\n",
    "# gradient wrt to a and gradient wrt b\n",
    "\n",
    "def gradient_hinge_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n",
    "\n",
    "def gradient_hinge_nll_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n",
    "\n",
    "def gradient_hinge_quadratic_error_loss(x, y, a, b):\n",
    "    # TODO TODO TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent algorithm\n",
    "\n",
    "You can now implement a gradient descent function that minimize any loss.\n",
    "You will need to code two variants:\n",
    "\n",
    "- stochastic=False: gradients are computed over the whole dataset\n",
    "- stochastic=True: gradients are approximated using a minibatch of size 1, similar to the previous algorithm.\n",
    "\n",
    "*Note: The stochastic gradient method is also available in ```sklearn``` under the name ```SGDClassify``` (SGD is the abbreviation for Stochastic Gradient Descent). A description is given on the page: http://scikit-learn.org/stable/modules/sgd.html.*\n",
    "\n",
    "Simple gradient descent is also called **batch**, and consists in calculating the true gradient $\\frac{1}{n} \\sum_{i=1}^n \\nabla_{\\mathbf{a}}\\ell(b +\\sum_{j=1}^d a_j [\\mathbf x_i]_j ; y_i)$. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, eps, niter, a_ini, b_ini, loss_fun, gr_loss_fun, stochastic=True):\n",
    "    \"\"\" Algorithm for gradient descent:\n",
    "        - X : Data\n",
    "        - y : label\n",
    "        - eps : learning rate\n",
    "        - niter : number of iterations\n",
    "        - a_ini, b_ini : initial weight\n",
    "        - loss_fun : cost function\n",
    "        - gr_loss_fun : gradient of the cost function \n",
    "        - stoch : True : implements SGD\n",
    "        \"\"\"\n",
    "    all_a = np.zeros((niter, a_ini.size))\n",
    "    all_b = np.zeros(niter)\n",
    "    \n",
    "    all_a[0] = a_ini\n",
    "    all_b[0] = b_ini\n",
    "    \n",
    "    # We also keep track of the loss and initialize it \n",
    "    loss = np.zeros(niter)\n",
    "    loss[0] = sum(loss_fun(X[i], y[i], all_a[0], all_b[0]) for i in range(X.shape[0])) / X.shape[0]\n",
    "    \n",
    "    for i in range(1, niter):\n",
    "        if stochastic: # Which indexes are we using in the SGD case ? \n",
    "            '''\n",
    "            Complete here\n",
    "            '''\n",
    "        else: # Which indexes are we using in the simple gradient case ? \n",
    "            '''\n",
    "            Complete here\n",
    "            '''\n",
    "        # Update the weight and compute the new loss\n",
    "        '''\n",
    "        Complete here\n",
    "        '''\n",
    "    return all_a, all_b, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different loss functions, plot the separating hyperplanes, etc.\n",
    "# as previously!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
