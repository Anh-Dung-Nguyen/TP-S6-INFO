{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic use of scikit-learn\n",
    "\n",
    "Scikit-learn is a well-known library for Machine Learning. It implements many basic \"methods\", and it is widely used in the community, both for research and in the industry.\n",
    "\n",
    "In this lab exercise, you will make the first steps with this library.\n",
    "\n",
    "Start by learning this introduction page: https://scikit-learn.org/1.4/tutorial/basic/tutorial.html\n",
    "\n",
    "**Note:** You may not known some methods that are discussed in the tutorial page. Don't focus on that but on the general philosophy of Scikit-learn.\n",
    "\n",
    "In the following, try to explore the data, different functions etc, by writting and executing a few lines of Python code! It is a good idea to always prototype simple example, to edit the code to check what works and what doesn't work, what are the shape of different tensors, what they contain, etc.\n",
    "\n",
    "Questions are there to help you identify what is important. Try to answer them, but you don't need to submit your answers.\n",
    "\n",
    "**WARNING:** The exam may contain questions about lab exercices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading pre-formated data\n",
    "\n",
    "We will use the IRIS dataset as a first example. You can read about this dataset on wikipedia: https://fr.wikipedia.org/wiki/Iris_de_Fisher\n",
    "\n",
    "The following bloc of code load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the IRIS dataset\n",
    "from sklearn.datasets import load_iris\n",
    "irisData=load_iris()\n",
    "# get info on the dataset\n",
    "#print(irisData.data)\n",
    "print(irisData.target)\n",
    "print(irisData.target_names)\n",
    "print(irisData.feature_names)\n",
    "#print(irisData.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** What type of machine learning problem is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** How many features are there? What kind of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt # replace the name \"pyplot\" by \"plt\" \n",
    "X=irisData.data\n",
    "y=irisData.target\n",
    "xi=0\n",
    "yi=1\n",
    "\n",
    "colors=[\"red\",\"green\",\"blue\"] # associate a color to each class label\n",
    "for num_label in range(3): # for each label\n",
    "    plt.scatter(\n",
    "        X[y==num_label][:, xi],\n",
    "        X[y==num_label][:,yi],\n",
    "        color=colors[num_label],\n",
    "        label=irisData.target_names[num_label]\n",
    "    )\n",
    "plt.legend()\n",
    "plt.xlabel(irisData.feature_names[xi]) \n",
    "plt.ylabel(irisData.feature_names[yi])\n",
    "plt.title(\"Iris Data - size of the sepals only\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** From the previous visualisation, what can you predict about the difficulty of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification with k-nearest neighbors\n",
    "\n",
    "We will now use a k-nearest neighbors classifier on this dataset. Start by reading the documentation page: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "And then study the code below.\n",
    "Try to use different values for k (called `nb_neighb` in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "nb_neighb = 15\n",
    "# to know more about the parameters, type help(neighbors.KNeighborsClassifier)\n",
    "clf = neighbors.KNeighborsClassifier(nb_neighb)\n",
    "\n",
    "clf.fit(X, y) # training\n",
    "print('accuracy on training data is', clf.score(X,y))\n",
    "\n",
    "# to predict on a specific example\n",
    "print('class predicted is', clf.predict([[ 5.4, 3.2, 1.6, 0.4]]))\n",
    "print('proba of each class is', clf.predict_proba([[ 5.4, 3.2, 1.6, 0.4]]))\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "print('misclassified training examples are:',X[y_pred!=y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** What kind of problem do you see with the evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 About training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want a test set and a training set, we can split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[0:100], y[0:100] # 100 examples for training\n",
    "X_test, y_test = X[100:], y[100:] # rest for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Explain why it is a really bad idea to split this iris dataset as we've done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a much better way to split the data into training and test sets.\n",
    "Again, start by reading the document page of the `train_test_split` function: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import random # to generate random numbers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=random.seed()\n",
    ")\n",
    "print(\n",
    "    'size of train / test = ',\n",
    "    len(X_train),\n",
    "    len(X_test)\n",
    ")\n",
    "\n",
    "print(\n",
    "    'nb of training data with class 0/1/2 =',\n",
    "    len(X_train[y_train==0]),\n",
    "    len(X_train[y_train==1]),\n",
    "    len(X_train[y_train==2])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now train and evaluate on two different parts of the original data.\n",
    "\n",
    "To display the results, we build a confusion matrix. Start by reading:\n",
    "\n",
    "- the wikipedia page: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "- the scikitlearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf=clf.fit(X_train, y_train)\n",
    "y_pred =clf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix\\n',cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** What is on the diagonal of the confusion matrix?\n",
    "\n",
    "**Q7:** What is the real error rate (give details)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K-fold\n",
    "\n",
    "The dataset is small, therefore there might be a high variance in test evaluation results.\n",
    "One solution to alleviate this issue, is to evaluate the model on different train/test splits.\n",
    "This approach is caleld k-fold.\n",
    "\n",
    "Start by reading the section about this approach in the documention: https://scikit-learn.org/stable/modules/cross_validation.html#k-fold\n",
    "\n",
    "And then try to understand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_folds = 10\n",
    "kf=KFold(n_splits=nb_folds,shuffle=True)\n",
    "score=0\n",
    "for training_ind,test_ind in kf.split(X):\n",
    "    #print(\"training index: \",training_ind,\"\\ntest index:\",test_ind,'\\n') \n",
    "    X_train=X[training_ind]\n",
    "    y_train=y[training_ind]\n",
    "    clf.fit(X_train, y_train)\n",
    "    X_test=X[test_ind]\n",
    "    y_test=y[test_ind]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = score + accuracy_score(y_pred,y_test)\n",
    "\n",
    "print('average accuracy:',score/nb_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as a one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "t_scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(t_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decision tree\n",
    "\n",
    "In this second part, we will build a decision tree using scikitlearn.\n",
    "Start by reading the documentation: https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "To read the data, we will use the pandas libraries, which simplify data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use another dataset (a CSV file). Pandas helps us to read this type of file.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = 'heart.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "\n",
    "features = X.columns\n",
    "classes = ['Not heart disease','heart disease']\n",
    "\n",
    "print (features)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=20,criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None,\n",
    "                                    feature_names=features,\n",
    "                                    class_names=classes,\n",
    "                                    filled=True, rounded=True))\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Graphviz is not working with your setup, look at http://people.irisa.fr/Vincent.Claveau/cours/fd/TP1.html\n",
    "\n",
    "**Q8:** Explain each line displayed in the nodes/leaves of the tree.\n",
    "    \n",
    "**Q9:** What is the name of this decision tree according to the course?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another nice viz of the decision tree. (The dtreeviz package is available in github. It can be installed with 'pip install dtreeviz'. It requires graphviz to be installed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dtreeviz.trees import dtreeviz # remember to load the package (depending on the library version, comment and uncomment lines)\n",
    "from dtreeviz import *\n",
    "#model or dtreeviz\n",
    "graph = model(clf, X_train, y_train,\n",
    "                target_name=\"target\",\n",
    "                feature_names=features,\n",
    "                class_names=classes\n",
    "                )\n",
    "\n",
    "#graph\n",
    "graph.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10:** Explain what are the histograms displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11** From the sklearn manual, explain what effectmax_depth or min_samples_split will have on the decision tree. If time permits, show the effects experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning Tmax\n",
    "\n",
    "Firsm check the documentation, again: https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n",
    "\n",
    "Here, we use a critrion called \"Cost Complexity\". Cost complexity pruning is all about finding the right parameter for alpha.We will get the alpha values for this tree and will check the accuracy with the pruned trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(ccp_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each alpha we will append our model to a list\n",
    "t_clf = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t_clf.append(clf)\n",
    "    \n",
    "# we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n",
    "t_clf = t_clf[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "node_counts = [clf.tree_.node_count for clf in t_clf]\n",
    "depth = [clf.tree_.max_depth for clf in t_clf]\n",
    "plt.scatter(ccp_alphas,node_counts)\n",
    "plt.scatter(ccp_alphas,depth)\n",
    "plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Tree complexity vs alpha')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# accuracy versus alpha\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for c in t_clf:\n",
    "    y_train_pred = c.predict(X_train)\n",
    "    y_val_pred = c.predict(X_val)\n",
    "    train_acc.append(accuracy_score(y_train_pred,y_train))\n",
    "    val_acc.append(accuracy_score(y_val_pred,y_val))\n",
    "\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,val_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,val_acc,label='val_accuracy',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12:** from the graph above, what is the best value for alpha. Replace it in the first line below ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = 0.12 # <-- replace this value\n",
    "clf_ = tree.DecisionTreeClassifier(random_state=0,ccp_alpha=best_alpha)\n",
    "clf_.fit(X_train,y_train)\n",
    "y_train_pred = clf_.predict(X_train)\n",
    "y_val_pred = clf_.predict(X_val)\n",
    "\n",
    "print('Train score', accuracy_score(y_train_pred,y_train))\n",
    "print(confusion_matrix(y_train_pred,y_train))\n",
    "\n",
    "print('Validation score', accuracy_score(y_val_pred,y_val))\n",
    "print(confusion_matrix(y_val_pred,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
